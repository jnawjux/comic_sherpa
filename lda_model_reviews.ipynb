{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (pyspark.sql.SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_review_df = spark.read.json('data/reviews_Books.json')\n",
    "comics_df = spark.read.json('data/comic_reviews_wtitle.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comics_asins = [asin[0] for asin in comics_df.select('asin').dropDuplicates().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_review_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering review text for comic books used from all book reviews\n",
    "comic_review_text  = book_review_df.select(['asin',\n",
    "                                            'reviewText',\n",
    "                                            'summary'\n",
    "                                           ]).filter(col(\"asin\").isin(comics_asins)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting individual reviews to dictionary of ASIN with all review text.\n",
    "review_text = {}\n",
    "for review in comic_review_text:\n",
    "    if review[0] not in review_text.keys():\n",
    "        review_text[review[0]] = review[1] + review[2]\n",
    "    else:\n",
    "        review_text[review[0]] += review[1] + review[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily saving to CSV\n",
    "reviews_df = pd.read_csv('data/review_text.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0316107255</td>\n",
       "      <td>PENGUIN DREAMS AND STRANGER THINGS is an engag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0345507460</td>\n",
       "      <td>Well, I'm sorry there's people who didn't enjo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0345529375</td>\n",
       "      <td>beautiful story with bryan's wonderful and awk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0375424148</td>\n",
       "      <td>The drawings are somtimes almost psychedelic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0375424334</td>\n",
       "      <td>If you like Chris Ware this is a must, the usu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                               text\n",
       "0  0316107255  PENGUIN DREAMS AND STRANGER THINGS is an engag...\n",
       "1  0345507460  Well, I'm sorry there's people who didn't enjo...\n",
       "2  0345529375  beautiful story with bryan's wonderful and awk...\n",
       "3  0375424148  The drawings are somtimes almost psychedelic, ...\n",
       "4  0375424334  If you like Chris Ware this is a must, the usu..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to clean and tokenize text\n",
    "def replace_punct_and_numbers(text):\n",
    "    \"\"\"Remove punctuation from document\"\"\"\n",
    "    punct = [punc for punc in string.punctuation]\n",
    "    num = list(range(10))\n",
    "    clean_text = \"\".join([letter for letter in text if (letter not in punct) and (letter not in num)])\n",
    "    return clean_text\n",
    "\n",
    "def remove_ner(text):\n",
    "    \"\"\"Remove NER words from text\"\"\"\n",
    "    doc_nlp = nlp(text)\n",
    "    ner_words = [word.text for word in doc_nlp.ents]\n",
    "    ner_regex = re.compile(\"|\".join(ner_words))\n",
    "    doc_ner_clean = ner_regex.sub('', text)\n",
    "    return doc_ner_clean\n",
    "\n",
    "def clean(doc):\n",
    "    \"\"\"Process text and return tokenized\"\"\"\n",
    "    punct_free = replace_punct_and_numbers(doc)\n",
    "    doc_ready = nlp(remove_ner(punct_free))\n",
    "    tokens = [token.lemma_.lower() for token in doc_ready \n",
    "                  if (not token.is_stop) and (token.text.strip() != \"\")]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [clean(doc) for doc in reviews_df['text'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.001*\"odst\" + 0.000*\"calender\" + 0.000*\"greenyellow\" + 0.000*\"index\" + 0.000*\"handbook\" + 0.000*\"ilver\" + 0.000*\"kindergartner\" + 0.000*\"adventures34\" + 0.000*\"ballet\" + 0.000*\"dorrie\" + 0.000*\"evans\" + 0.000*\"omega\" + 0.000*\"helljumper\" + 0.000*\"stein\" + 0.000*\"cole\" + 0.000*\"nova\" + 0.000*\"carlet\" + 0.000*\"hadow\" + 0.000*\"2099\" + 0.000*\"odsts\" + 0.000*\"madame\" + 0.000*\"vas\" + 0.000*\"baron\" + 0.000*\"adorableness\" + 0.000*\"thembirthday\"'), (1, '0.024*\"story\" + 0.024*\"book\" + 0.014*\"read\" + 0.014*\"not\" + 0.012*\"s\" + 0.010*\"like\" + 0.010*\"comic\" + 0.009*\"good\" + 0.009*\"great\" + 0.008*\"character\" + 0.008*\"art\" + 0.007*\"series\" + 0.007*\"comic_strip\" + 0.006*\"love\" + 0.006*\"fan\" + 0.006*\"time\" + 0.005*\"novel\" + 0.005*\"volume\" + 0.005*\"work\" + 0.005*\"issue\" + 0.005*\"graphic\" + 0.005*\"look\" + 0.005*\"page\" + 0.004*\"buy\" + 0.004*\"come\"'), (2, '0.020*\"story\" + 0.017*\"s\" + 0.014*\"not\" + 0.012*\"character\" + 0.011*\"book\" + 0.011*\"good\" + 0.010*\"like\" + 0.008*\"issue\" + 0.008*\"read\" + 0.008*\"series\" + 0.008*\"new\" + 0.006*\"volume\" + 0.006*\"great\" + 0.006*\"time\" + 0.005*\"end\" + 0.005*\"go\" + 0.005*\"come\" + 0.004*\"get\" + 0.004*\"know\" + 0.004*\"art\" + 0.004*\"hero\" + 0.004*\"thing\" + 0.004*\"think\" + 0.004*\"have\" + 0.004*\"team\"'), (3, '0.003*\"dale\" + 0.003*\"album\" + 0.002*\"lion\" + 0.002*\"immigrant\" + 0.002*\"que\" + 0.001*\"de\" + 0.001*\"ic\" + 0.001*\"gunslinger\" + 0.001*\"katet\" + 0.001*\"la\" + 0.001*\"en\" + 0.001*\"los\" + 0.001*\"el\" + 0.001*\"es\" + 0.001*\"immigration\" + 0.001*\"squire\" + 0.000*\"zoo\" + 0.000*\"hedge\" + 0.000*\"calligraphy\" + 0.000*\"cd\" + 0.000*\"y\" + 0.000*\"las\" + 0.000*\"una\" + 0.000*\"environmental\" + 0.000*\"harem\"'), (4, '0.002*\"di\" + 0.001*\"e\" + 0.001*\"che\" + 0.001*\"il\" + 0.001*\"nd\" + 0.001*\"tht\" + 0.001*\"una\" + 0.001*\"un\" + 0.001*\"del\" + 0.000*\"tp\" + 0.000*\"piugrave\" + 0.000*\"egrave\" + 0.000*\"la\" + 0.000*\"da\" + 0.000*\"le\" + 0.000*\"ma\" + 0.000*\"della\" + 0.000*\"hve\" + 0.000*\"si\" + 0.000*\"anche\" + 0.000*\"dei\" + 0.000*\"con\" + 0.000*\"questa\" + 0.000*\"questo\" + 0.000*\"beta\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=5, num_words=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.save('data/lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.8621955), (2, 0.13542761)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_document_topics(doc_term_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel_3topics = Lda(doc_term_matrix, num_topics = 3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.022*\"story\" + 0.018*\"book\" + 0.014*\"not\" + 0.014*\"s\" + 0.011*\"read\" + 0.010*\"good\" + 0.010*\"like\" + 0.010*\"character\" + 0.008*\"great\" + 0.007*\"series\" + 0.007*\"issue\" + 0.007*\"comic\" + 0.006*\"art\" + 0.006*\"volume\" + 0.006*\"time\" + 0.005*\"new\" + 0.004*\"love\" + 0.004*\"comic_strip\" + 0.004*\"fan\" + 0.004*\"work\" + 0.004*\"come\" + 0.004*\"get\" + 0.004*\"know\" + 0.004*\"look\" + 0.004*\"think\"'), (1, '0.010*\"vampire\" + 0.002*\"dale\" + 0.002*\"chef\" + 0.002*\"food\" + 0.002*\"album\" + 0.001*\"sushi\" + 0.001*\"rman\" + 0.001*\"di\" + 0.001*\"rgirl\" + 0.001*\"mandias\" + 0.001*\"foodie\" + 0.001*\"che\" + 0.001*\"87\" + 0.000*\"restaurant\" + 0.000*\"culinary\" + 0.000*\"il\" + 0.000*\"vamp\" + 0.000*\"odst\" + 0.000*\"e\" + 0.000*\"una\" + 0.000*\"211\" + 0.000*\"jeanclaude\" + 0.000*\"un\" + 0.000*\"outlaw\" + 0.000*\"del\"'), (2, '0.001*\"immigrant\" + 0.001*\"de\" + 0.001*\"que\" + 0.001*\"la\" + 0.001*\"nd\" + 0.001*\"en\" + 0.001*\"los\" + 0.000*\"el\" + 0.000*\"threeboote\" + 0.000*\"es\" + 0.000*\"calendar\" + 0.000*\"tht\" + 0.000*\"una\" + 0.000*\"immigration\" + 0.000*\"del\" + 0.000*\"las\" + 0.000*\"di\" + 0.000*\"un\" + 0.000*\"wordless\" + 0.000*\"y\" + 0.000*\"como\" + 0.000*\"para\" + 0.000*\"se\" + 0.000*\"por\" + 0.000*\"hve\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel_3topics.print_topics(num_topics=3, num_words=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9998981)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel_3topics.get_document_topics(doc_term_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = gensim.models.LsiModel(doc_term_matrix, num_topics=3, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.382*\"story\" + 0.375*\"book\" + 0.287*\"not\" + 0.287*\"read\" + 0.248*\"s\" + 0.191*\"like\" + 0.178*\"character\" + 0.178*\"good\" + 0.155*\"comic\" + 0.148*\"great\"'),\n",
       " (1,\n",
       "  '0.395*\"story\" + 0.322*\"s\" + -0.294*\"book\" + -0.268*\"read\" + -0.245*\"tv\" + -0.237*\"series\" + -0.231*\"zombie\" + -0.216*\"compendium\" + -0.169*\"love\" + -0.154*\"comic\"'),\n",
       " (2,\n",
       "  '0.389*\"novel\" + 0.319*\"book\" + 0.311*\"graphic\" + -0.308*\"issue\" + -0.268*\"volume\" + -0.239*\"series\" + 0.235*\"story\" + -0.163*\"new\" + -0.131*\"s\" + -0.124*\"great\"')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=3, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
