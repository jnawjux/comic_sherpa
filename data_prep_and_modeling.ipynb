{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import pyspark\n",
    "import databricks.koalas as ks\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from batcave.scrape_and_clean import new_id_dictionary, get_missing_titles\n",
    "from batcave.recommend import get_recommendations, get_user_reviews_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (pyspark.sql.SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading comic and movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "comic_reviews = ks.read_json('data/comic_reviews_wtitle.json')\n",
    "movie_reviews = ks.read_json('data/movies_and_videos_wtitle.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new ids\n",
    "Since the Amazon user and item ids contain both letters and numbers, I needed to give them new values for a couple reasons:\n",
    "* The Spark ALS model will only take labels that are numeric and will cause an error otherwise.\n",
    "* I can create ids for the comic books and movies/tv that are easily identifiable and easy to filter on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_ids = new_id_dictionary(comic_reviews, 'reviewerID', '00')\n",
    "new_comic_asins = new_id_dictionary(comic_reviews, 'asin', '22')\n",
    "new_mtv_asins = new_id_dictionary(movie_reviews, 'asin', '44')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items_asins = new_comic_asins\n",
    "all_items_asins.update(new_mtv_asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comic_reviews['item_id'] = comic_reviews.asin.map(all_items_asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfUserId = F.udf(lambda x: new_user_ids[x], StringType())\n",
    "udfItemId = F.udf(lambda x: all_items_asins[x], StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comic_reviews_updated = comic_reviews.withColumn(\"item_id\", udfItemId(\"asin\"))\n",
    "comic_reviews_updated = comic_reviews_updated.withColumn(\"user_id\", udfUserId(\"reviewerID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+-------------+--------------------+-------+-------+\n",
      "|      asin|               imUrl|overall|   reviewerID|               title|item_id|user_id|\n",
      "+----------+--------------------+-------+-------------+--------------------+-------+-------+\n",
      "|0345507460|http://ecx.images...|    5.0|ACO26JQ366659|The Dresden Files...| 236422| 771300|\n",
      "+----------+--------------------+-------+-------------+--------------------+-------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comic_reviews_updated.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews_updated = movie_reviews.withColumn(\"item_id\", udfItemId(\"asin\"))\n",
    "movie_reviews_updated = movie_reviews_updated.withColumn(\"user_id\", udfUserId(\"reviewerID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+-------------+--------------------+-------+-------+\n",
      "|      asin|               imUrl|overall|   reviewerID|               title|item_id|user_id|\n",
      "+----------+--------------------+-------+-------------+--------------------+-------+-------+\n",
      "|0767807693|http://ecx.images...|    3.0|ADENUJJYKNHPO|Requiem for a Hea...|1464744| 794500|\n",
      "+----------+--------------------+-------+-------------+--------------------+-------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_updated.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrowing my dataset\n",
    "To help improve the accuracy of my model, I wanted to look through removing some of the items and users that may not offer much insight because of their sparsity. Here I explored some of these features to help me decide on what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing users who have rated less than average in either comics or movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    item_id \n",
    ",   COUNT(*) as count \n",
    "FROM \n",
    "    table \n",
    "GROUP BY item_id\n",
    "ORDER BY count desc\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comic_reviews_updated.createOrReplaceTempView('table')\n",
    "comic_low_reviewers = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of items with less than average amount of comic reviews: 4129\n"
     ]
    }
   ],
   "source": [
    "low_reviews = comic_low_reviewers[comic_low_reviewers['count'] \\\n",
    "                                  <= comic_low_reviewers['count'].mean()]\n",
    "print(f\"Amount of items with less than average amount of comic reviews: {low_reviews.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews_updated.createOrReplaceTempView('table')\n",
    "mtv_low_reviewers = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of items with less than average amount of movie reviews: 28082\n"
     ]
    }
   ],
   "source": [
    "low_mtv_reviews = mtv_low_reviewers[mtv_low_reviewers['count'] \\\n",
    "                                  <= mtv_low_reviewers['count'].mean()]\n",
    "print(f\"Amount of items with less than average amount of movie reviews: {low_mtv_reviews.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5444</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39044</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2077544</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2097544</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2326944</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  count\n",
       "0     5444    170\n",
       "1    39044    163\n",
       "2  2077544    141\n",
       "3  2097544    140\n",
       "4  2326944    133"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtv_low_reviewers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32211"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_item = list(set(low_reviews['item_id'].tolist() + low_mtv_reviews['item_id'].tolist()))\n",
    "len(low_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining and adding review counts\n",
    "all_review_counts_df = pd.concat([mtv_low_reviewers,comic_low_reviewers])\n",
    "all_review_counts = spark.createDataFrame(all_review_counts_df)\n",
    "\n",
    "# Joining comic & movie data\n",
    "all_reviews = comic_reviews_updated.union(movie_reviews_updated)\n",
    "\n",
    "# Adding the count column\n",
    "all_reviews = all_reviews.join(all_review_counts, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+-------+--------------+--------------------+-------+-----+\n",
      "|item_id|      asin|               imUrl|overall|    reviewerID|               title|user_id|count|\n",
      "+-------+----------+--------------------+-------+--------------+--------------------+-------+-----+\n",
      "|1003644|6301661192|http://ecx.images...|    1.0|A1D2C0WDCSHUWZ|      Meridian [VHS]|  81600|    1|\n",
      "| 101122|0785115277|http://ecx.images...|    2.0|A169I83JL8QJGN|Excalibur Vol. 1:...| 824100|    5|\n",
      "| 101122|0785115277|http://ecx.images...|    1.0|A1AM5YLSD7H0CM|Excalibur Vol. 1:...| 205000|    5|\n",
      "| 101122|0785115277|http://ecx.images...|    4.0| AJKWF4W7QD4NS|Excalibur Vol. 1:...| 702000|    5|\n",
      "| 101122|0785115277|http://ecx.images...|    1.0| AJ8L5D48CSEB4|Excalibur Vol. 1:...| 683800|    5|\n",
      "+-------+----------+--------------------+-------+--------------+--------------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_ready = all_reviews.filter(F.col('item_id').isin(low_item) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-------+----------+-----+-------+-----+\n",
      "|item_id|asin|imUrl|overall|reviewerID|title|user_id|count|\n",
      "+-------+----+-----+-------+----------+-----+-------+-----+\n",
      "|   7872|7872| 7362|      5|      8565| 5293|   8565|  105|\n",
      "+-------+----+-----+-------+----------+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seeing the unique count of users and items\n",
    "all_reviews_ready.agg(*(F.countDistinct(F.col(c)).alias(c) for c in all_reviews_ready.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting to temporarily preserve\n",
    "all_reviews_ready.repartition(1).write.json(\"data/all_reviews_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case of the missing titles\n",
    "Even though I had limited my set to movies/tv that have metadata, I found that some of the metadata is missing the title for movies, which is a pretty important piece for my further development!  Since I do have the ASINs, I wrote a quick function to find those missing titles by querying Amazon and then returning the first only results title. My process was as follows:\n",
    "* Get the ASINs for data missing titles\n",
    "* Run function on those ASINs to return a title\n",
    "* Do some clean up of titles\n",
    "* Add titles back to original data and drop old listing\n",
    "* Export all_reviews again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = spark.read.json('data/all_reviews_new.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the ASINS from products missing titles\n",
    "missing_titles = all_reviews.select(['asin','title']).toPandas()\n",
    "\n",
    "missing_asins = list(set(missing_titles.loc[missing_titles['title'].isna(), 'asin'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Records missing name: 2556\n"
     ]
    }
   ],
   "source": [
    "print(f\" Records missing name: {len(missing_asins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a function to scrape Amazon for the title - Takes a few hours to complete\n",
    "missing_title_info = get_missing_titles(missing_asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop these into a dataframe to inspect\n",
    "missing_df = pd.DataFrame(missing_title_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B004MSP04C</td>\n",
       "      <td>None found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B002IVNLKA</td>\n",
       "      <td>World's Greatest Dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B002ZG99WW</td>\n",
       "      <td>Yogi Bear (2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000OCY7JE</td>\n",
       "      <td>After the Wedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B003ND0AXS</td>\n",
       "      <td>Disney Phineas &amp; Ferb: A Very Perry Christmas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                          title\n",
       "0  B004MSP04C                                     None found\n",
       "1  B002IVNLKA                           World's Greatest Dad\n",
       "2  B002ZG99WW                               Yogi Bear (2010)\n",
       "3  B000OCY7JE                              After the Wedding\n",
       "4  B003ND0AXS  Disney Phineas & Ferb: A Very Perry Christmas"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df.to_csv('data/missing_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = pd.read_csv('data/missing_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still had some that returned no name, so ignoring for now and will drop from all reviews\n",
    "all_the_good = missing_df[missing_df['title'] != 'None found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B002IVNLKA</th>\n",
       "      <td>1</td>\n",
       "      <td>World's Greatest Dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B002ZG99WW</th>\n",
       "      <td>2</td>\n",
       "      <td>Yogi Bear (2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B000OCY7JE</th>\n",
       "      <td>3</td>\n",
       "      <td>After the Wedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B003ND0AXS</th>\n",
       "      <td>4</td>\n",
       "      <td>Disney Phineas &amp; Ferb: A Very Perry Christmas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B003L77G7Y</th>\n",
       "      <td>5</td>\n",
       "      <td>American Dad!, Vol. 6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0                                          title\n",
       "asin                                                                 \n",
       "B002IVNLKA           1                           World's Greatest Dad\n",
       "B002ZG99WW           2                               Yogi Bear (2010)\n",
       "B000OCY7JE           3                              After the Wedding\n",
       "B003ND0AXS           4  Disney Phineas & Ferb: A Very Perry Christmas\n",
       "B003L77G7Y           5                          American Dad!, Vol. 6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting ASIN as index to do replace with original set\n",
    "all_the_good.set_index('asin', inplace=True)\n",
    "missing_titles.set_index('asin', inplace=True)\n",
    "all_the_good.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining to replace missing titles\n",
    "missing_titles = missing_titles.combine_first(all_the_good)\n",
    "\n",
    "# Resetting the index and changing title column name \n",
    "missing_titles.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of all without a title still\n",
    "check_missing_asins = list(set(missing_df.loc[missing_titles['title'].isna(), 'asin'].tolist()))\n",
    "\n",
    "# Dropping rows with reviews that have no title from original dataframe\n",
    "all_reviews_less_missing_titles = all_reviews.filter(F.col('asin').isin(check_missing_asins)==False)\n",
    "\n",
    "# Dropping rows from my temporary dataframe with correct names\n",
    "fix_titles = missing_titles.drop(missing_titles[missing_titles['title'].isna()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some text cleanup on titles. There may be more later, but these are the initial examples I found:\n",
    "potential_regs = \"\"\"\\[VHS\\]|\\[DVD\\]|Collector\\'s Edition|\\: Season \\d+\n",
    "                    |\\: The Complete Series|\\: Complete Series|\\(.*\\)\"\"\"\n",
    "\n",
    "fix_titles['title'] = fix_titles['title']\\\n",
    "                         .apply(lambda x: re.sub(potential_regs, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005119367</td>\n",
       "      <td>Joseph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005119367</td>\n",
       "      <td>Joseph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0005119367</td>\n",
       "      <td>Joseph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005119367</td>\n",
       "      <td>Joseph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0307142469</td>\n",
       "      <td>Frosty the Snowman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                title\n",
       "0  0005119367              Joseph \n",
       "1  0005119367              Joseph \n",
       "2  0005119367              Joseph \n",
       "3  0005119367              Joseph \n",
       "4  0307142469  Frosty the Snowman "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_titles.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "fix_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark dataframe from dataframe with corrected titles\n",
    "fixed_titles_spark = spark.createDataFrame(fix_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining & exporting\n",
    "all_reviews_no_title = all_reviews_less_missing_titles.select(['asin',\n",
    "                                                               'count',\n",
    "                                                               'imUrl',\n",
    "                                                               'item_id',\n",
    "                                                               'overall',\n",
    "                                                               'reviewerID',\n",
    "                                                               'user_id'])\n",
    "\n",
    "all_reviews_with_fixed_titles = all_reviews_no_title.join(fixed_titles_spark,\n",
    "                                                                     on='asin',\n",
    "                                                                     how='left').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+-------+-------+--------------+-------+--------------------+\n",
      "|      asin|count|               imUrl|item_id|overall|    reviewerID|user_id|               title|\n",
      "+----------+-----+--------------------+-------+-------+--------------+-------+--------------------+\n",
      "|0345507460|   57|http://ecx.images...| 236422|    5.0| ACO26JQ366659| 771300|The Dresden Files...|\n",
      "|0345507460|   57|http://ecx.images...| 236422|    5.0|A34C35QFA4DC5J| 808700|The Dresden Files...|\n",
      "|0345507460|   57|http://ecx.images...| 236422|    5.0|A3TII4RKU0ZVT4| 446700|The Dresden Files...|\n",
      "|0345507460|   57|http://ecx.images...| 236422|    5.0|A1LR4Z5Z0MPYIF| 818200|The Dresden Files...|\n",
      "|0345507460|   57|http://ecx.images...| 236422|    5.0|A16L43DIFSHGMQ| 758300|The Dresden Files...|\n",
      "|0345507460|   57|http://ecx.images...| 236422|    5.0| ANRS196NKFVUU|  53100|The Dresden Files...|\n",
      "|0345507460|   57|http://ecx.images...| 236422|    1.0|A30BEBBQ3UYI7O| 611900|The Dresden Files...|\n",
      "|0345507460|   57|http://ecx.images...| 236422|    5.0|A1UI62FWXYH1T5| 472600|The Dresden Files...|\n",
      "|0345507460|   57|http://ecx.images...| 236422|    5.0|A20J9MBDENFXMU| 890100|The Dresden Files...|\n",
      "|0345507460|   57|http://ecx.images...| 236422|    4.0|A18XZ92NCH14QZ| 358500|The Dresden Files...|\n",
      "+----------+-----+--------------------+-------+-------+--------------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_reviews_with_fixed_titles.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews_with_fixed_titles.repartition(1).write.json('data/all_reviews_fixed_titles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews  = spark.read.json('data/all_reviews_fixed_titles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_ready = all_reviews.select([F.col(\"user_id\").cast(IntegerType()),\n",
    "                                  F.col(\"item_id\").cast(IntegerType()),\n",
    "                                  F.col(\"overall\").cast(IntegerType()),\n",
    "                                  F.col(\"title\"), \n",
    "                                  F.col(\"imUrl\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS\n",
    "als = ALS(rank=50, regParam=.1, maxIter=20,\n",
    "          userCol='user_id', itemCol='item_id', \n",
    "          ratingCol='overall', nonnegative=True)\n",
    "\n",
    "als_model = als.fit(als_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.1743178289845608\n",
      "Test MAE: 0.9355244490473343\n"
     ]
    }
   ],
   "source": [
    "test_pred_df = als_model.transform(test).toPandas()\n",
    "\n",
    "# Filling in NaN values with average score for metric review\n",
    "test_pred_df['prediction'].fillna(4, inplace=True)\n",
    "test_pred = spark.createDataFrame(test_pred_df)\n",
    "\n",
    "# Get RMSE & MAE for model\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"overall\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "evaluator_2 = RegressionEvaluator(metricName=\"mae\", labelCol=\"overall\",\n",
    "                                predictionCol=\"prediction\")\n",
    "\n",
    "rmse_test = evaluator.evaluate(test_pred)\n",
    "mae_test = evaluator_2.evaluate(test_pred)\n",
    "print(f\"Test RMSE: {rmse_test}\")\n",
    "print(f\"Test MAE: {mae_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning for optimization\n",
    "I kept running into errors with my parameter grid to cross validate below, so ran several tests as well varying parameters and landed on my best model using the configuration found in the above model test.  Below is an example of the parameters this was based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(userCol='user_id', itemCol='item_id', ratingCol='overall', nonnegative=True)\n",
    "\n",
    "reg_test = RegressionEvaluator(predictionCol='prediction', labelCol='overall')\n",
    "\n",
    "# Parameter grid              \n",
    "params = ParamGridBuilder().addGrid(als.regParam, [0.01,0.001,0.1])\\\n",
    "                           .addGrid(als.rank, [4,10,50])\\\n",
    "                           .addGrid(als.maxIter, [5,10,15,20]).build()\n",
    "             \n",
    "## Calling and checking evaluator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=params,evaluator=reg_test,parallelism=4)\n",
    "best_model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning recommendations\n",
    "To get recommendations for new users, I first need to get the item features from my ALS model, save them with titles, and then can use that file to create and build recommendations.\n",
    "Below is an example of the two functions created to perform the recommendation operation, just on the Notebook level:\n",
    "* ```get_user_reviews_testing```: gives an individual a selection of movies to choose from, once they have rated at least 10, it creates and returns a dataframe with their scores.\n",
    "* ```get_recommedations```: returns the top 10 comic books recommended to the user based on their input.\n",
    "\n",
    "These functions will be used as the basis for creating a functional web application for users to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS\n",
    "als = ALS(rank=50, regParam=.1, maxIter=20,\n",
    "          userCol='user_id', itemCol='item_id', \n",
    "          ratingCol='overall', nonnegative=True)\n",
    "\n",
    "als_model = als.fit(als_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get item factors from model\n",
    "item_factors = als_model.itemFactors.toPandas()\n",
    "item_factors.rename(columns={'id': 'item_id'}, inplace=True)\n",
    "item_factors['item_id'] = item_factors['item_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get titles from original dataset\n",
    "item_titles = all_reviews.select(['item_id', 'title', 'count', 'asin']).distinct().toPandas()\n",
    "item_titles['item_id'] = item_titles['item_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into one dataframe & making the ids string for exploration later\n",
    "item_details = item_factors.merge(item_titles, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'CSV data source does not support array<double> data type.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o489.csv.\n: org.apache.spark.sql.AnalysisException: CSV data source does not support array<double> data type.;\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$$anonfun$verifySchema$1.apply(DataSourceUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$$anonfun$verifySchema$1.apply(DataSourceUtils.scala:67)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifySchema(DataSourceUtils.scala:67)\n\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifyWriteSchema(DataSourceUtils.scala:34)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:100)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-79748dbf5074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Exporting to preserve item factor details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexport_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexport_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/als_item_factor_details'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue)\u001b[0m\n\u001b[1;32m    925\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                        encoding=encoding, emptyValue=emptyValue)\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.3/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'CSV data source does not support array<double> data type.;'"
     ]
    }
   ],
   "source": [
    "# Exporting to preserve item factor details\n",
    "export_df = spark.createDataFrame(item_details)\n",
    "export_df.repartition(1).write.csv('data/als_item_factor_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7872, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_details.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test = get_user_reviews_testing(item_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>715044</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2934544</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>858944</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2306644</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1453244</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  rating\n",
       "0   715044       4\n",
       "1  2934544       3\n",
       "2   858944       4\n",
       "3  2306644       2\n",
       "4  1453244       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = get_recommendations(item_factors_df=item_details, new_user_df=user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>asin</th>\n",
       "      <th>new_user_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>183222</td>\n",
       "      <td>Batman Illustrated by Neal Adams: Volume 3</td>\n",
       "      <td>1401204074</td>\n",
       "      <td>4.855964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>255522</td>\n",
       "      <td>Ultimate X-Men Vol. 1: The Tomorrow People</td>\n",
       "      <td>0785107886</td>\n",
       "      <td>4.512662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>198322</td>\n",
       "      <td>Captain America: The Art of Captain America - ...</td>\n",
       "      <td>0785155090</td>\n",
       "      <td>4.232017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>119222</td>\n",
       "      <td>Secret Wars</td>\n",
       "      <td>078511873X</td>\n",
       "      <td>4.187856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>177222</td>\n",
       "      <td>Batman: Vampire</td>\n",
       "      <td>1401215653</td>\n",
       "      <td>4.152080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    item_id                                              title        asin  \\\n",
       "462  183222         Batman Illustrated by Neal Adams: Volume 3  1401204074   \n",
       "637  255522         Ultimate X-Men Vol. 1: The Tomorrow People  0785107886   \n",
       "496  198322  Captain America: The Art of Captain America - ...  0785155090   \n",
       "295  119222                                       Secret Wars   078511873X   \n",
       "447  177222                                    Batman: Vampire  1401215653   \n",
       "\n",
       "     new_user_predictions  \n",
       "462              4.855964  \n",
       "637              4.512662  \n",
       "496              4.232017  \n",
       "295              4.187856  \n",
       "447              4.152080  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! This finishes my data prep and modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding how to fold a new user into the model\n",
    "Below is a walkthrough of getting recommendations for a user by using the existing item features and the ratings from the new user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting item features from the model and setting the index to the item_id\n",
    "item_factors = als_model.itemFactors.toPandas()\n",
    "item_factors.index = item_factors['id']\n",
    "item_factors['id'] = item_factors['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>[0.27627408504486084, 0.3660491704940796, 0.22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>1344</td>\n",
       "      <td>[0.21467094123363495, 0.25732994079589844, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>1544</td>\n",
       "      <td>[0.3672632575035095, 0.28194597363471985, 0.35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>2044</td>\n",
       "      <td>[0.28777071833610535, 0.301589697599411, 0.131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>2244</td>\n",
       "      <td>[0.08827913552522659, 0.32467201352119446, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           features\n",
       "id                                                           \n",
       "144    144  [0.27627408504486084, 0.3660491704940796, 0.22...\n",
       "1344  1344  [0.21467094123363495, 0.25732994079589844, 0.0...\n",
       "1544  1544  [0.3672632575035095, 0.28194597363471985, 0.35...\n",
       "2044  2044  [0.28777071833610535, 0.301589697599411, 0.131...\n",
       "2244  2244  [0.08827913552522659, 0.32467201352119446, 0.1..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking for some movies to rate, all movies id ends in 44\n",
    "item_factors.loc[item_factors['id'].str.endswith('44')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1344</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1544</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2244</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  rating\n",
       "0   144       3\n",
       "1  1344       3\n",
       "2  1544       3\n",
       "3  2244       3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a random user with a set of ratings\n",
    "user = [{'id': 144, 'rating': 3}, {'id': 1344, 'rating': 3}, \n",
    "        {'id': 1544, 'rating': 3}, {'id': 2244, 'rating': 3}]\n",
    "user_df = pd.DataFrame(user)\n",
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of each column to calculate user matrix\n",
    "item_ids = user_df.id.tolist()\n",
    "user_rating = user_df.rating.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User ratings \n",
    "all_ratings_array = np.array((user_rating,)).T\n",
    "\n",
    "# Get item features for these specific movies\n",
    "all_items_array = np.zeros(shape=(len(item_ids), 50))\n",
    "\n",
    "for index, item in enumerate(item_ids):\n",
    "    all_items_array[index, :] = np.array(item_factors.loc[item, 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 1), (4, 50))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of each to make sure things are looking right\n",
    "all_ratings_array.shape, all_items_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Least squares solution to get user features\n",
    "new_user_matrix = np.linalg.lstsq(all_items_array, all_ratings_array, rcond=None)\n",
    "\n",
    "# New users matrix!\n",
    "new_user_matrix = new_user_matrix[0].reshape((50,))\n",
    "new_user_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.999999999999999"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking based on known score\n",
    "known_item = np.array(item_factors.loc[144,'features'])\n",
    "\n",
    "score = np.dot(new_user_matrix, known_item)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now I am going to user that factor dataframe to create predictions for this user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors['new_user_predictions'] = item_factors['features'].apply(lambda x: np.dot(x, new_user_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five_comics = item_factors.loc[item_factors['id']\\\n",
    "                              .str.endswith('22'), 'new_user_predictions']\\\n",
    "                              .sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "311422    3.620749\n",
       "112922    3.546200\n",
       "89922     3.486923\n",
       "315022    3.477472\n",
       "70322     3.475644\n",
       "Name: new_user_predictions, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_five_comics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! I can now functionize this principle and apply to getting speedy recommendations for my new users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
